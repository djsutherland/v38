<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title><span>Minimizing Nonconvex Non-Separable Functions</span> | AISTATS 2015 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="{Minimizing Nonconvex Non-Separable Functions}">

  <meta name="citation_author" content="Yu, Yaoliang">

  <meta name="citation_author" content="Zheng, Xun">

  <meta name="citation_author" content="Marchetti-Bowick, Micol">

  <meta name="citation_author" content="Xing, Eric">

<meta name="citation_publication_date" content="2015">
<meta name="citation_conference_title" content="Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics">
<meta name="citation_firstpage" content="1107">
<meta name="citation_lastpage" content="1115">
<meta name="citation_pdf_url" content="yu15.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1><span>Minimizing Nonconvex Non-Separable Functions</span></h1>

	<div id="authors">
	
		Yaoliang Yu,
	
		Xun Zheng,
	
		Micol Marchetti-Bowick,
	
		Eric Xing
	<br />
	</div>
	<div id="info">
		Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics,
		pp. 1107–1115, 2015
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		Regularization has played a key role in deriving sensible estimators in high dimensional statistical inference. A substantial amount of recent works has argued for nonconvex regularizers in favor of their superior theoretical properties and excellent practical performances. In a different but analogous vein, nonconvex loss functions are promoted because of their robustness against “outliers”. However, these nonconvex formulations are computationally more challenging, especially in the presence of nonsmoothness and non-separability. To address this issue, we propose a new proximal gradient meta-algorithm by rigorously extending the proximal average to the nonconvex setting. We formally prove its nice convergence properties, and illustrate its effectiveness on two applications: multi-task graph-guided fused lasso and robust support vector machines. Experiments demonstrate that our method compares favorably against other alternatives.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="yu15.pdf">Download PDF</a></li>
			
			<li><a href="yu15-supp.pdf">Supplementary (PDF)</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
