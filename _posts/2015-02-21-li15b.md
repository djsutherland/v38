---
supplementary: http://proceedings.mlr.press/v38/li15b-supp.pdf
title: Toward Minimax Off-policy Value Estimation
abstract: This paper studies the off-policy evaluation problem, where one aims to
  estimate the value of a target policy based on a sample of observations collected
  by another policy.  We first consider the multi-armed bandit case, establish a finite-time
  minimax risk lower bound, and analyze the risk of three standard estimators.  It
  is shown that in a large class of settings the so-called regression estimator is
  minimax optimal up to a constant that depends on the number of actions, while the
  other two can be arbitrarily worse even in the limit of infinitely many data points,
  despite their empirical success and popularity.  The performance of these estimators
  are studied in synthetic and real problems; illustrating the nontriviality of this
  simple task.  Finally the results are extended to the problem of off-policy evaluation
  in contextual bandits and fixed-horizon Markov decision processes.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: li15b
month: 0
tex_title: "{Toward Minimax Off-policy Value Estimation}"
firstpage: 608
lastpage: 616
page: 608-616
order: 608
cycles: false
author:
- given: Lihong
  family: Li
- given: Remi
  family: Munos
- given: Csaba
  family: Szepesvari
date: 2015-02-21
address: San Diego, California, USA
publisher: PMLR
container-title: Proceedings of the Eighteenth International Conference on Artificial
  Intelligence and Statistics
volume: '38'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 2
  - 21
pdf: http://proceedings.mlr.press/v38/li15b.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
