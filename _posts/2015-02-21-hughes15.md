---
supplementary: http://proceedings.mlr.press/v38/hughes15-supp.pdf
title: Reliable and Scalable Variational Inference for the Hierarchical Dirichlet
  Process
abstract: We introduce a new variational inference objective for hierarchical Dirichlet
  process admixture models. Our approach provides novel and scalable algorithms for
  learning nonparametric topic models of text documents and Gaussian admixture models
  of image patches. Improving on the point estimates of topic probabilities used in
  previous work, we define full variational posteriors for all latent variables and
  optimize parameters via a novel surrogate likelihood bound. We show that this approach
  has crucial advantages for data-driven learning of the number of topics. Via merge
  and delete moves that remove redundant or irrelevant topics, we learn compact and
  interpretable models with less computation. Scaling to millions of documents is
  possible using stochastic or memoized variational updates.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: hughes15
month: 0
tex_title: "{Reliable and Scalable Variational Inference for the Hierarchical Dirichlet
  Process}"
firstpage: 370
lastpage: 378
page: 370-378
order: 370
cycles: false
author:
- given: Michael
  family: Hughes
- given: Dae Il
  family: Kim
- given: Erik
  family: Sudderth
date: 2015-02-21
address: San Diego, California, USA
publisher: PMLR
container-title: Proceedings of the Eighteenth International Conference on Artificial
  Intelligence and Statistics
volume: '38'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 2
  - 21
pdf: http://proceedings.mlr.press/v38/hughes15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
